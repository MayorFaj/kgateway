# EP-11500: KGateway Load Testing Framework

* Issue: [#11210](https://github.com/kgateway-dev/kgateway/issues/11210)

<!-- toc -->
* [Background](#background)
* [Motivation](#motivation)
* [Goals](#goals)
* [Non-Goals](#non-goals)
* [Implementation Details](#implementation-details)
  * [Test Framework Structure](#test-framework-structure)
  * [Core Test Scenarios](#core-test-scenarios)
    * [Attached Routes Test](#attached-routes-test)
    * [Route Probe Test](#route-probe-test)
    * [Route Change Test](#route-change-test)
  * [Gateway-API-Bench Integration](#gateway-api-bench-integration)
  * [Enhanced Scale Testing](#enhanced-scale-testing)
  * [CI/CD Integration](#cicd-integration)
    * [Test Registration](#test-registration)
    * [Makefile Targets](#makefile-targets)
  * [Test Plan](#test-plan)
    * [Unit Tests](#unit-tests)
    * [Integration Tests](#integration-tests)
    * [Performance Baselines](#performance-baselines)
* [Alternatives](#alternatives)
  * [Alternative 1: Ginkgo E2E Test Integration (PROPOSED)](#alternative-1-ginkgo-e2e-test-integration-proposed)
  * [Alternative 2: Standalone Load Testing Tool](#alternative-2-standalone-load-testing-tool)
* [Open Questions](#open-questions)
  * [Performance Thresholds and Baselines](#performance-thresholds-and-baselines)
  * [Test Coverage and Scenarios](#test-coverage-and-scenarios)
  * [CI/CD Integration Strategy](#cicd-integration-strategy)
  * [Technical Implementation](#technical-implementation)
<!-- /toc -->

## Background

This EP proposes adding a comprehensive load testing framework for KGateway to ensure performance and reliability under various load conditions. The framework will integrate into KGateway's existing Ginkgo-based e2e test infrastructure.

This enhancement addresses GitHub issue [#11210](https://github.com/kgateway-dev/kgateway/issues/11210), which requests automated scale tests for kgateway that can run in developer environments and as part of build/release workflows. The existing legacy performance test workflows need to be replaced with a modern, integrated solution.

Performance testing is essential for Gateway API implementations as they handle critical traffic routing decisions. Current testing focuses primarily on functional correctness, leaving performance characteristics largely unvalidated. A comprehensive load testing framework will help identify performance bottlenecks, ensure scalability, and prevent regressions in production deployments.

The proposed framework will provide standardized performance testing patterns that can be consistently applied across KGateway development cycles, enabling reliable performance validation and regression detection both locally and in CI/CD pipelines.

## Motivation

Performance testing is crucial for Gateway API implementations as they handle critical traffic routing decisions. The existing Gateway API conformance tests only cover basic functionality, leaving performance characteristics largely untested. Real-world deployments require gateways that can:

1. Handle thousands of routes without performance degradation
2. Maintain traffic availability during configuration changes
3. Provide consistent response times under varying load conditions
4. Scale control plane operations efficiently

Without comprehensive load testing, KGateway risks encountering production performance issues, including memory leaks, route propagation delays, and traffic disruptions during route changes.

## Goals

The following list defines goals for this EP:

* Implement a comprehensive load testing framework for KGateway based on the proven gateway-api-bench pattern
* Integrate load testing into KGateway's existing Ginkgo-based e2e test infrastructure
* Provide three core test scenarios: Attached Routes, Route Probe, and Route Change tests
* Enable automated performance regression testing in CI/CD pipelines
* Generate detailed performance metrics and reports for analysis
* Establish performance baselines and alerts for regression detection

## Non-Goals

The following list defines non-goals for this EP:

* Provide production traffic load testing capabilities
* Support non-Gateway API load testing scenarios
* Create a general-purpose load testing framework for arbitrary applications
* Support real-time traffic analysis or APM capabilities

## Implementation Details

### Test Framework Structure

Add load testing as a new feature suite within KGateway's existing e2e test infrastructure:

```text
test/kubernetes/e2e/features/
  load_testing/
    suite.go                    # Main Ginkgo test suite
    attachedroutes_test.go      # Attached routes load test
    probe_test.go               # Route probe load test
    routechange_test.go         # Route change load test
    pkg/
      simulation/
        runner.go               # Test execution logic adapted from gateway-api-bench
        metrics.go              # Performance metrics collection
        watcher.go              # Gateway status monitoring
      testdata/
        manifests/
          backend.yaml          # Test backend services
```

### Core Test Scenarios

Following the gateway-api-bench pattern, implemented as Ginkgo test cases:

#### Attached Routes Test

* **Purpose**: Measure time for route attachment status to be reflected in Gateway status
* **Implementation**:
  * Ginkgo test that monitors Gateway `attachedRoutes` status using Kubernetes client
  * Apply HTTPRoutes via KGateway's existing test helpers
  * Track timing between route creation and status update
* **Metrics**: Route attachment time, status propagation delay, total writes
* **Success Criteria**: All routes attached within configurable time threshold

#### Route Probe Test

* **Purpose**: Measure route propagation time from configuration to traffic acceptance
* **Implementation**:
  * Ginkgo test that applies HTTPRoutes sequentially via test framework
  * Use HTTP client to probe endpoints until 200 responses
  * Track propagation latency and error rates
* **Metrics**: Route propagation latency, error count, maximum response time
* **Success Criteria**: Routes become traffic-ready within acceptable latency bounds

#### Route Change Test

* **Purpose**: Ensure traffic continuity during route configuration changes
* **Implementation**:
  * Ginkgo test that generates continuous HTTP traffic using test helpers
  * Apply route configuration changes during traffic
  * Monitor request success rates and response times
* **Metrics**: Request success rate, latency distribution, error count during changes
* **Success Criteria**: Zero traffic drops, consistent response times during changes

### Gateway-API-Bench Integration

The implementation includes a framework for comparative analysis against gateway-api-bench benchmarks:

* **Integrated Benchmark Tests**: Run gateway-api-bench benchmarks as part of KGateway's test suite
* **Comparative Metrics Reporting**: Generate reports comparing KGateway performance against benchmarks
* **Regression Detection**: Identify performance regressions relative to benchmark results

### Enhanced Scale Testing

In addition to the core scenarios, the framework supports enhanced scale testing to evaluate KGateway performance under high-load conditions:

* **Large-Scale Route Management**: Test gateway performance with thousands of routes
* **High-Concurrency Traffic Simulation**: Generate and manage high levels of concurrent traffic
* **Resource Utilization Monitoring**: Track CPU, memory, and network usage during tests

### CI/CD Integration

#### Test Registration

```go
// In test/kubernetes/e2e/tests/kgateway_tests.go
func KubeGatewaySuiteRunner() e2e.SuiteRunner {
    // ...existing code...
    kubeGatewaySuiteRunner.Register("LoadTesting", load_testing.NewTestingSuite)
    // ...existing code...
}
```

#### Makefile Targets

```makefile
.PHONY: load-test
load-test:
    TEST_PKG=./test/kubernetes/e2e/features/load_testing make test

.PHONY: load-test-ci
load-test-ci:
    SKIP_INSTALL=true TEST_PKG=./test/kubernetes/e2e/features/load_testing make test
```

### Test Plan

#### Unit Tests

* Test simulation framework components (runner, metrics, watcher)
* Mock Gateway and HTTPRoute resource handling
* Metrics collection and reporting logic
* Performance baseline calculations

#### Integration Tests

* End-to-end load test scenarios within KGateway's existing e2e framework
* Integration with KGateway's test installation and cleanup
* Performance regression detection
* CI/CD pipeline integration testing

#### Performance Baselines

* Establish baseline performance metrics for KGateway
* Automated regression detection comparing current vs. baseline results
* Performance trend analysis over time
* Alert thresholds for CI/CD failures

## Alternatives

### Alternative 1: Ginkgo E2E Test Integration (PROPOSED)

* **Pros**:
  * Proven automation and testing methodology from gateway-api-bench
  * Integrates seamlessly with KGateway's existing Ginkgo test infrastructure
  * Runs automatically as part of established CI/CD pipelines
  * Comprehensive test coverage for Gateway API specific scenarios
  * Real-time monitoring and detailed metrics collection
  * Configurable thresholds and performance scoring
* **Cons**: Required adapting gateway-api-bench logic to Ginkgo patterns
* **Decision**: **PROPOSED** - Leverages proven patterns within existing infrastructure

### Alternative 2: Standalone Load Testing Tool

* **Pros**:
  * Could provide more specialized load testing features
  * Independent of KGateway's test infrastructure
  * Could support multiple gateway implementations
* **Cons**:
  * Would require separate CI/CD integration
  * No integration with existing KGateway test utilities
  * Additional maintenance overhead
  * Less alignment with established testing patterns
* **Decision**: Rejected - Integration benefits outweigh standalone advantages

## Open Questions

The following questions remain open for further discussion and resolution:

### Performance Thresholds and Baselines

1. **Setup and Teardown Time Targets**:
   * What should be the target setup time for 1000 routes?
   * Should teardown time have the same threshold as setup time?
   * Should thresholds scale based on route count
   * What's the acceptable performance degradation for multi-gateway scenarios?

2. **Production Scale Requirements**:
   * What are the target production scales we're optimizing for?
   * Maximum expected routes per gateway in production environments?
   * Should we test different route counts (100, 500, 1000, 5000)?
   * What's the expected number of gateways per cluster in production?

3. **Resource and Concurrency Limits**:
   * What's the optimal concurrency level to avoid API server overload?
   * Should concurrency scale with cluster size/resources?
   * What are the Kubernetes API server rate limits we need to respect?
   * Minimum cluster requirements for running load tests?

### Test Coverage and Scenarios

1. **Monitoring and Metrics Collection**:
   * What metrics should be collected during load tests?
   * Should we monitor memory/CPU usage of kgateway components?
   * Do we need to track API server response times?
   * Should we measure end-to-end latency (creation to traffic-ready)?

### CI/CD Integration Strategy

1. **Pipeline Integration**:
   * Should load tests run in CI for every PR or only on specific triggers?
   * What's the acceptable test duration for CI environments?
   * Should we have different test profiles (quick vs comprehensive)?
   * How do we handle test environment variability across CI systems?

2. **Environment Adaptation**:
   * Should tests adapt automatically to cluster capacity?
   * Do we need to test on different Kubernetes versions?
   * Should we test with different kgateway configurations?
   * How should performance thresholds be configurable per environment?

### Technical Implementation

1. **Framework Architecture**:
   * Should we implement a simulation framework similar to gateway-api-bench patterns?
   * What's the preferred approach: bulk operations vs batched operations?
   * How many concurrent workers should be used for route creation/deletion?
   * Should we use server-side apply or standard create operations?

2. **Baseline Establishment and Regression Detection**:
   * How should initial performance baselines be established?
   * What hardware/cluster configurations should be used for baselines?
   * What criteria should trigger performance regression alerts?
   * Where should historical performance data be stored?

3. **Result Persistence and Reporting**:
   * How will performance metrics integrate with existing e2e test reporting?
   * Should results be uploaded to centralized performance tracking systems?
   * What format should be used for performance reports?
   * How will trends and comparisons be visualized?
